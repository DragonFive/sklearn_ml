{
 "metadata": {
  "name": "",
  "signature": "sha256:2d1c48029f952bf1f0d3bb81438775c9b0ffa202ef5e4c8a12d4d810edb1d16d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# \u7279\u5f81\u63d0\u53d6\u548c\u5904\u7406 \n",
      "---\n",
      "\n",
      "## \u79bb\u6563\u53d8\u91cf\u7279\u5f81\u63d0\u53d6\n",
      "\n",
      "scikit-learn\u91cc\u6709DictVectorizer \u7c7b\u53ef\u4ee5\u7528\u6765\u8868\u793a\u79bb\u6563\u7279\u5f81\uff0c\u79bb\u6563\u53d8\u91cf\u901a\u5e38\u7528\u72ec\u70ed\u7f16\u7801(One-of-K or One-Hot Encoding),\u901a\u8fc7\u4e8c\u8fdb\u5236\u6570\u6765\n",
      "\u8868\u793a\u6bcf\u4e2a\u89e3\u91ca\u53d8\u91cf\u7684\u7279\u5f81.\u7528DictVectorizer\u8fdb\u884c\u7279\u5f81\u8f6c\u6362\uff0c\u628aDict\u8f6c\u5316\u4f4d\u4e8c\u8fdb\u5236.\n",
      "\n",
      "\u4ece**\u7279\u5f81\u63d0\u53d6\u6a21\u5757**\u91cc\u4f7f\u7528\u79bb\u6563\u5316\u5de5\u5177"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "onehot_encoder = DictVectorizer()\n",
      "instances = [{'city':'New York'},{'city':'San Francisco'},{'city':'Chapel Hill'}]\n",
      "onehot_encoder.fit_transform(instances).todense()# \u5f97\u5230\u4e09\u4e2a\u503c\u5bf9\u5e94\u7684\u4e8c\u8fdb\u5236\u5f62\u5f0f\n",
      "onehot_encoder.vocabulary_\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "{'city=Chapel Hill': 0, 'city=New York': 1, 'city=San Francisco': 2}"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u4f1a\u770b\u5230,\u7f16\u7801\u7684\u4f4d\u7f6e\u5e76\u4e0d\u662f\u4e0e\u4e0a\u9762\u57ce\u5e02\u4e00\u4e00\u5bf9\u5e94\u7684\u3002\u7b2c\u4e00\u4e2acity \u7f16\u7801New York \u662f[ 0. 1. 0.] ,\u7528\u7b2c\u4e8c\u4e2a\u5143\u7d20\u4e3a1 \u8868\u793a\u3002\u76f8\u6bd4\u7528\u5355\u72ec\u7684\u6570\u503c\u6765\u8868\u793a\u5206\u7c7b,\u8fd9\u79cd\u65b9\u6cd5\u770b\u8d77\u6765\u5f88\u76f4\u89c2\u3002New York , SanFrancisco , Chapel Hill \u53ef\u4ee5\u8868\u793a\u62101 ,2 ,3 \u3002**\u6570\u503c\u7684\u5927\u5c0f\u6ca1\u6709\u5b9e\u9645\u610f\u4e49,\u57ce\u5e02\u5e76\u6ca1\u6709\u81ea\u7136\u6570\u987a\u5e8f**\u3002\n",
      "\n",
      "---\n",
      "## \u6587\u5b57\u7279\u5f81\u63d0\u53d6\n",
      "\u5f88\u591a\u673a\u5668\u5b66\u4e60\u95ee\u9898\u6d89\u53ca\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP),\u5fc5\u7136\u8981\u5904\u7406\u6587\u5b57\u4fe1\u606f\u3002\u6587\u5b57\u5fc5\u987b\u8f6c\u6362\u6210\u53ef\u4ee5\u91cf\u5316\u7684\u7279\u5f81\u5411\u91cf\u3002\u4e0b\u9762\u6211\u4eec\u5c31\u6765\u4ecb\u7ecd\u6700\u5e38\u7528\u7684\u6587\u5b57\u8868\u793a\u65b9\u6cd5:**\u8bcd\u5e93\u6a21\u578b(Bag-of-words model)**\u3002\n",
      "\n",
      "### \u8bcd\u5e93\u8868\u793a\u6cd5 \n",
      "\n",
      "\u628a\u6587\u7ae0\u770b\u505a\u662f\u4e00\u4e2a\u8bcd\u96c6\u5408,\u6216\u8005\u8bf4\u662f\u8bcd\u7684\u4e00\u4e2a\u7ec4\u5408,\u6587\u6863\u4e2d**\u6bcf\u4e2a\u8bcd\u7684\u51fa\u73b0\u90fd\u662f\u72ec\u7acb\u7684**,\u4e0d\u4f9d\u8d56\u4e8e\u5176\u4ed6\u8bcd\u662f\u5426\u51fa\u73b0\u3002\u8bcd\u5e93\u6a21\u578b\u53ef\u4ee5\u770b\u6210\u662f\u72ec\u70ed\u7f16\u7801\u7684\u4e00\u79cd\u6269\u5c55\uff0c\u5b83\u4e3a\u6bcf\u4e2a\u5355\u8bcd\u8bbe\u7f6e\u4e00\u4e2a\u7279\u5f81\u503c\u3002\u8bcd\u5e93\u6a21\u578b\u4f9d\u636e\u662f\u7528**\u7c7b\u4f3c\u5355\u8bcd\u7684\u6587\u7ae0\u610f\u601d\u4e5f\u5dee\u4e0d\u591a**\u3002\u8bcd\u5e93\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u6709\u9650\u7684\u7f16\u7801\u4fe1\u606f\u5b9e\u73b0\u6709\u6548\u7684**\u6587\u6863\u5206\u7c7b\u548c\u68c0\u7d22**\u3002\n",
      "\n",
      "\u4e00\u6279\u6587\u6863\u7684\u96c6\u5408\u79f0\u4e3a\u6587\u96c6(corpus)\n",
      "\n",
      "\u4e0b\u9762\u7684\u6587\u96c6\u5305\u62ec8\u4e2a\u8bcd:UNC , played , Duke , in , basketball , lost , the , game \u3002\u6587\u96c6\u7684\u5355\u8bcd\u6784\u6210\u8bcd\u6c47\u8868(vocabulary)\u3002\u8bcd\u5e93\u6a21\u578b\u7528\u6587\u96c6\u7684\u8bcd\u6c47\u8868\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u7684\u7279\u5f81\u5411\u91cf\u8868\u793a\u6bcf\u4e2a\u6587\u6863\u3002\u6211\u4eec\u7684\u6587\u96c6\u67098\u4e2a\u5355\u8bcd,\u90a3\u4e48\u6bcf\u4e2a\u6587\u6863\u5c31\u662f\u7531**\u4e00\u4e2a\u5305\u542b8\u4f4d\u5143\u7d20\u7684\u5411\u91cf**(\u8868\u793a\u6bcf\u4e2a\u5355\u8bcd\u662f\u5426\u5b58\u5728\u8fd9\u4e2a\u6587\u6863\u91cc\u9762)\u6784\u6210\u3002\u6784\u6210**\u7279\u5f81\u5411\u91cf\u7684\u5143\u7d20\u6570\u91cf\u79f0\u4e3a\u7ef4\u5ea6**(dimension)\u3002\u7528\u4e00\u4e2a\u8bcd\u5178(dictionary)\u6765\u8868\u793a**\u8bcd\u6c47\u8868\u4e0e\u7279\u5f81\u5411\u91cf\u7d22\u5f15**\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\n",
      "\n",
      "\u5728\u5927\u591a\u6570\u8bcd\u5e93\u6a21\u578b\u4e2d,\u7279\u5f81\u5411\u91cf\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u662f\u7528\u4e8c\u8fdb\u5236\u6570(0/1)\u8868\u793a\u5355\u8bcd\u662f\u5426\u5728\u6587\u6863\u4e2d\u3002**CountVectorizer** \u7c7b\u4f1a\u628a\u6587\u6863\u5168\u90e8\u8f6c\u6362\u6210\u5c0f\u5199,\u7136\u540e\u5c06**\u6587\u6863\u8bcd\u5757\u5316(tokenize)**\u3002\u6587\u6863\u8bcd\u5757\u5316\u662f\u628a\u53e5\u5b50\u5206\u5272\u6210\u8bcd\u5757(token)\u6216\u6709\u610f\u4e49\u7684\u5b57\u6bcd\u5e8f\u5217\u7684\u8fc7\u7a0b\u3002\u8bcd\u5757\u5927\u591a\u662f\u5355\u8bcd,\u4f46\u662f\u4ed6\u4eec\u4e5f\u53ef\u80fd\u662f\u4e00\u4e9b\u77ed\u8bed,\u5982\u6807\u70b9\u7b26\u53f7\u548c\u8bcd\u7f00\u3002CountVectorizer \u7c7b\u901a\u8fc7**\u6b63\u5219\u8868\u8fbe\u5f0f\u7528\u7a7a\u683c\u5206\u5272\u53e5\u5b50,\u7136\u540e\u62bd\u53d6\u957f\u5ea6\u5927\u4e8e\u7b49\u4e8e2\u7684\u5b57\u6bcd\u5e8f\u5217**\u505a\u5355\u8bcd\u3002"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "corpus = [\n",
      "        'UNC played Duke in basketball',\n",
      "        'Duke lost the basketball game',\n",
      "        'I ate a sandwich'\n",
      "          ]\n",
      "vectorizer = CountVectorizer()\n",
      "\n",
      "# \u4e0b\u9762\u5bf9\u6587\u96c6\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u540c\u65f6\u521b\u5efa\u5b57\u5178\uff0ctodense() \u53ef\u4ee5\u628a\u6587\u96c6\u5185\u5bb9\u53d8\u5f97\u53ef\u4ee5\u8f93\u51fa\n",
      "vectorizer.fit_transform(corpus).todense()\n",
      "vectorizer.vocabulary_\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "{u'ate': 0,\n",
        " u'basketball': 1,\n",
        " u'duke': 2,\n",
        " u'game': 3,\n",
        " u'in': 4,\n",
        " u'lost': 5,\n",
        " u'played': 6,\n",
        " u'sandwich': 7,\n",
        " u'the': 8,\n",
        " u'unc': 9}"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u901a\u8fc7CountVectorizer \u7c7b\u53ef\u4ee5\u5f97\u51fa\u4e0a\u9762\u7684\u7ed3\u679c\u3002\u8bcd\u6c47\u8868\u91cc\u9762\u670910\u4e2a\u5355\u8bcd,\u4f46I\u548ca \u4e0d\u5728\u8bcd\u6c47\u8868\u91cc\u9762,\u662f\u56e0\u4e3aI\u548ca \u7684**\u957f\u5ea6\u4e0d\u7b26\u5408CountVectorizer \u7c7b\u7684\u8981\u6c42**\u3002\n",
      "\n",
      "---\n",
      "## \u76f8\u4f3c\u6027\u5ea6\u91cf\n",
      "\n",
      "\u53ef\u4ee5\u7528\u6b27\u5f0f\u8ddd\u79bb\u8ba1\u7b97\u6587\u96c6\u91cc\u9762\u4e24\u4e2a\u6587\u6863\u7684\u76f8\u4f3c\u5ea6\u3002\u4e24\u5411\u91cf\u7684\u6b27\u5f0f\u8ddd\u79bb\u5c31\u662f\u4e24\u4e2a\u5411\u91cf\u7684\u6b27\u5f0f\u8303\u6570\u6216\u8005\u53eb\u505a$L_2$\u8303\u6570\u5dee\u7684\u7edd\u5bf9\u503c\u3002\n",
      "\n",
      "scikit-learn\u91cc\u9762\u7684**euclidean_distances** \u51fd\u6570\u53ef\u4ee5\u8ba1\u7b97\u82e5\u5e72\u5411\u91cf\u7684\u8ddd\u79bb,\u8868\u793a\u4e24\u4e2a\u8bed\u4e49\u6700\u76f8\u4f3c\u7684\u6587\u6863\u5176\u5411\u91cf\u5728\u7a7a\u95f4\u4e2d\u4e5f\u662f\u6700\u63a5\u8fd1\u7684\u3002"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics.pairwise import euclidean_distances\n",
      "# \u5148\u5f97\u5230\u5404\u4e2a\u6587\u6863\u7684\u7279\u5f81\u5411\u91cf \n",
      "counts = vectorizer.fit_transform(corpus).todense()\n",
      "print 'docs likes'\n",
      "print euclidean_distances(counts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "docs likes\n",
        "[[ 0.          2.44948974  2.64575131]\n",
        " [ 2.44948974  0.          2.64575131]\n",
        " [ 2.64575131  2.64575131  0.        ]]\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u5982\u679c\u6211\u4eec\u7528\u65b0\u95fb\u62a5\u9053\u5185\u5bb9\u505a\u6587\u96c6,\u8bcd\u6c47\u8868\u5c31\u53ef\u4ee5\u7528**\u6210\u5343\u4e0a\u4e07\u4e2a\u5355\u8bcd**\u3002\u6bcf\u7bc7\u65b0\u95fb\u7684\u7279\u5f81\u5411\u91cf\u90fd\u4f1a\u6709\u6210\u5343\u4e0a\u4e07\u4e2a\u5143\u7d20,\u5f88\u591a\u5143\u7d20\u90fd\u4f1a\u662f0\u3002\u4f53\u80b2\u65b0\u95fb\u4e0d\u4f1a\u5305\u542b\u8d22\u7ecf\u65b0\u95fb\u7684\u672f\u8bed,\u540c\u6837\u6587\u5316\u65b0\u95fb\u4e5f\u4e0d\u4f1a\u5305\u542b\u8d22\u7ecf\u65b0\u95fb\u7684\u672f\u8bed\u3002\u6709\u8bb8\u591a\u96f6\u5143\u7d20\u7684**\u9ad8\u7ef4\u7279\u5f81\u5411\u91cf\u6210\u4e3a\u7a00\u758f\u5411\u91cf**(sparse vectors)\u3002\u7528\u9ad8\u7ef4\u6570\u636e\u53ef\u4ee5\u91cf\u5316\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u65f6\u4f1a\u6709\u4e00\u4e9b\u95ee\u9898,\u4e0d\u53ea\u662f\u51fa\u73b0\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u3002\u7b2c\u4e00\u4e2a\u95ee\u9898\u5c31\u662f**\u9ad8\u7ef4\u5411\u91cf\u9700\u8981\u5360\u7528\u66f4\u5927\u5185\u5b58**\u3002NumPy\u63d0\u4f9b\u4e86\u4e00\u4e9b\u6570\u636e\u7c7b\u578b\u53ea\u663e\u793a\u7a00\u758f\u5411\u91cf\u7684\u975e\u96f6\u5143\u7d20,\u53ef\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e2a\u95ee\u9898\u3002\n",
      "\n",
      "\u7b2c\u4e8c\u4e2a\u95ee\u9898\u5c31\u662f\u8457\u540d\u7684\u7ef4\u5ea6\u707e\u96be(curse of dimensionality,Hughes effect),**\u7ef4\u5ea6\u8d8a\u591a\u5c31\u8981\u6c42\u66f4\u5927\u7684\u8bad\u7ec3\u96c6\u6570\u636e**\u4fdd\u8bc1\u6a21\u578b\u80fd\u591f\u5145\u5206\u5b66\u4e60\u3002\u5982\u679c\u8bad\u7ec3\u6837\u672c\u4e0d\u591f,\u90a3\u4e48\u7b97\u6cd5\u5c31\u53ef\u80fd\u62df\u5408\u8fc7\u5ea6\u5bfc\u81f4\u5f52\u7eb3\u5931\u8d25\u3002\n",
      "\n",
      "\u4e0b\u9762\u662f\u4e00\u4e9b\u964d\u7ef4\u7684\u65b9\u6cd5\u3002\u5728\u540e\u9762PCA\u964d\u7ef4\u91cc\u9762,\u7528\u7684\u662f\u6570\u503c\u65b9\u6cd5\u964d\u7ef4\u3002\n",
      "\n",
      "---\n",
      "\n",
      "## \u505c\u7528\u8bcd\u8fc7\u6ee4\n",
      "\n",
      "**\u6587\u96c6\u505c\u7528\u8bcd**(stop-word),\u50cfa ,an ,the ,\u52a9\u52a8\u8bcddo ,be ,will ,\u4ecb\u8bcdon ,around ,beneath \u7b49\u3002**\u505c\u7528\u8bcd\u901a\u5e38\u662f\u6784\u5efa\u6587\u6863\u610f\u601d\u7684\u529f\u80fd\u8bcd\u6c47**,\u5176\u5b57\u9762\n",
      "\u610f\u4e49\u5e76\u4e0d\u4f53\u73b0\u3002CountVectorizer \u7c7b\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6estop_words \u53c2\u6570\u8fc7\u6ee4\u505c\u7528\u8bcd,\u9ed8\u8ba4\u662f\u82f1\u8bed\u5e38\u7528\u7684\u505c\u7528\u8bcd\u3002\n",
      "\n",
      "\u770b\u4e0b\u9762\u8bbe\u7f6e\u4e86\u505c\u7528\u8bcd\u4e4b\u540e\uff0c\u5b57\u5178\u91cc\u7684\u5143\u7d20\u5c31\u5c11\u4e86\uff0c\u6bcf\u4e2a\u6587\u6863\u751f\u6210\u7684\u7279\u5f81\u5411\u91cf\u5c31\u5c11\u4e86 in / he \u7b49"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(stop_words='english') # \u8bbe\u7f6e\u505c\u7528\u8bcd\u4f4d\u9ed8\u8ba4\u82f1\u8bed\u91cc\u9762\u7684,\u4e0d\u8bbe\u7f6e\u5c31\u6ca1\u6709\n",
      "vectorizer.fit_transform(corpus).todense()\n",
      "vectorizer.vocabulary_\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "{u'ate': 0,\n",
        " u'basketball': 1,\n",
        " u'duke': 2,\n",
        " u'game': 3,\n",
        " u'lost': 4,\n",
        " u'played': 5,\n",
        " u'sandwich': 6,\n",
        " u'unc': 7}"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "## \u8bcd\u6839\u8fd8\u539f\u4e0e\u8bcd\u5f62\u8fd8\u539f\n",
      "\n",
      "\n",
      "\u8bcd\u6839\u8fd8\u539f\u4e0e\u8bcd\u5f62\u8fd8\u539f\u5c31\u662f\u4e3a\u4e86\u5c06\u5355\u8bcd\u4ece\u4e0d\u540c\u7684\u65f6\u6001\u3001\u6d3e\u751f\u5f62\u5f0f\u8fd8\u539f\u3002\n",
      "\n",
      "\u4e0b\u9762\u662f\u6ca1\u6709\u7528\u8bcd\u6839\u8bcd\u5f62\u8fd8\u539f\u7684\u60c5\u51b5"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus2 = [\n",
      "        'he ate the sandwiches',\n",
      "        'every sandwich was eaten by him'\n",
      "        ]\n",
      "vectorizer = CountVectorizer(stop_words='english')\n",
      "features = vectorizer.fit_transform(corpus2).todense()\n",
      "print vectorizer.vocabulary_\n",
      "print features\n",
      "print euclidean_distances(features)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'sandwich': 2, u'ate': 0, u'sandwiches': 3, u'eaten': 1}\n",
        "[[1 0 0 1]\n",
        " [0 1 1 0]]\n",
        "[[ 0.  2.]\n",
        " [ 2.  0.]]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u4ece\u4e0a\u9762\u53ef\u4ee5\u770b\u51fa\u6ca1\u6709\u8bcd\u6839\u8bcd\u5f62\u8fd8\u539f\uff0c\u4ed6\u4eec\u4e4b\u95f4\u7684\u5dee\u522b\u8fd8\u662f\u5f88\u5927\u7684\u3002\n",
      "\n",
      "\u8bcd\u5f62\u8fd8\u539f\u901a\u5e38\u9700\u8981**\u8bcd\u6cd5\u8d44\u6599**\u7684\u652f\u6301,\u6bd4\u5982WordNet\u548c\u5355\u8bcd\u8bcd\u7c7b(part of speech)\u3002\u8bcd\u6839\u8fd8\u539f\u7b97\u6cd5\u901a\u5e38\u9700\u8981\u7528**\u89c4\u5219\u4ea7\u751f\u8bcd\u5e72(stem)**\u5e76\u64cd\u4f5c\u8bcd\u5757,\u4e0d\u9700\u8981\u8bcd\u6cd5\u8d44\u6e90\n",
      "\n",
      "\n",
      "\u6211\u4eec\u7528Python\u7684NLTK(Natural Language Tool Kit \u81ea\u7136\u8bed\u8a00\u5de5\u5177\u5305)(http://www.nltk.org/install.html)\u5e93\u6765\u5904\u7406\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.download() # \u4e0b\u8f7dnltk\u7684\u6570\u636e\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### \u8bcd\u5f62\u8fd8\u539f \n",
      "\n",
      "\u4e0b\u9762\u6211\u4eec\u7528\u8bcd\u6cd5\u8d44\u6599\u6765\u5206\u6790\u4e00\u4e0b\u5355\u8bcdgathering\u7684\u8bcd\u5f62\u8fd8\u539f \n",
      "\n",
      "\u7b2c\u4e00\u53e5\u7684gathering \u662f\u52a8\u8bcd,\u5176\u8bcd\u5143\u662fgather \u3002\u540e\u4e00\u53e5\u7684gathering \u662f\u540d\u8bcd,\u5176\u8bcd\u5143\u662fgathering \u3002\u6211\u4eec\u7528Python\u7684NLTK(Natural Language Tool Kit)\n",
      "(http://www.nltk.org/install.html)\u5e93\u6765\u5904\u7406\u3002"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "print (lemmatizer.lemmatize('gathering', 'v'))\n",
      "print (lemmatizer.lemmatize('gathering', 'n'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gather\n",
        "gathering\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u4e0a\u9762\u6211\u4eec\u53d1\u73b0\u53ef\u4ee5\u7528\u4e00\u4e2a\u5355\u8bcd\u5728\u53e5\u5b50\u4e2d\u7684\u8bcd\u6027\u4f5c\u4e3a\u53c2\u6570\u4f7f\u7528wordnet \u6765\u83b7\u5f97\u4ed6\u4eec\u7684\u8bcd\u5f62\u3002\u4f46\u662f\u6211\u4eec\u600e\u4e48\u6839\u636e\u8bed\u5883\u6765\u83b7\u5f97\u5b83\u4eec\u7684\u8bcd\u6027\u5462\uff1f \u7528nltk.stem\u91cc\u9762\u7684porterstemmer\u5bf9\u8c61\u6765\u63a8\u6d4b\u8bcd\u6027"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}